{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the GELU activation function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi, dtype=x.dtype, device=x.device)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralMNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut): #layer size indicates how many neurons each layer should contain.\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        #\n",
    "        self.layers = nn.ModuleList([ #constructing the neural network\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1], GELU())),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2], GELU())),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3], GELU())),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4], GELU())),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5], GELU()))\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            #compute the output of the current layer\n",
    "            layer_output = layer(x) #the output of the first layer serves as input to the second layer\n",
    "            #check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1] #five layers with 3 neuron and the last layer with one neuron\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralMNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the gradients for every layer\n",
    "def print_gradients(model, x):\n",
    "    #forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    #calculate loss based on how close the target and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    #backward pass to calculate the loss gradients for each layer\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            #print the mean absolute gradient of the weughts\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0015313407639041543\n",
      "layers.1.0.weight has gradient mean of 0.0008734685834497213\n",
      "layers.2.0.weight has gradient mean of 0.002111609559506178\n",
      "layers.3.0.weight has gradient mean of 0.0030934568494558334\n",
      "layers.4.0.weight has gradient mean of 0.007880656979978085\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a clear represntation of the varnishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.24866610765457153\n",
      "layers.1.0.weight has gradient mean of 0.8006523251533508\n",
      "layers.2.0.weight has gradient mean of 0.3836197853088379\n",
      "layers.3.0.weight has gradient mean of 0.3954206109046936\n",
      "layers.4.0.weight has gradient mean of 1.001085877418518\n"
     ]
    }
   ],
   "source": [
    "#setting the shortcut to be true\n",
    "model_with_shortcut = ExampleDeepNeuralMNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustration clearly shows how the shortcut connection solves the varbishing gradients problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
