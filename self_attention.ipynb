{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#getting the vectors for the input sequence\n",
    "inputs =  torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], #Your x1\n",
    "        [0.55, 0.87, 0.66], #journey x2\n",
    "        [0.57, 0.85, 0.64], #starts x3\n",
    "        [0.22, 0.58, 0.33], #with x4\n",
    "        [0.77, 0.25, 0.10], #one  x5\n",
    "        [0.05, 0.80, 0.55]  #step x6\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining variables\n",
    "x_2 = inputs[2] #second input element, query\n",
    "d_in = inputs.shape[1] #row dimension of the weights matrix, it must be set to the col dimention of the input/query embedding\n",
    "d_out = 2 #col dimension of the weights matrix, it can be set to any thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the query, key and value weight matrices\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) \n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) \n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the `requires_grad` to `False` here means that the values of the weights will not be optimized as the model is being trained. When we are building the model fr, we will set the vaue to true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in GPT models the output dims are usually the same, but for the sake of this practical, we are making them different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)\n",
    "print(W_key)\n",
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4300, 1.4343])\n",
      "tensor([0.4361, 1.1156])\n",
      "tensor([0.3879, 0.9831])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "print(key_2)\n",
    "print(value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Queries: torch.Size([6, 2])\n",
      "Shape of Keys: torch.Size([6, 2])\n",
      "Shape of values: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "#on=btaining the querries, keys and values matrices\n",
    "queries = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(f\"Shape of Queries: {queries.shape}\")\n",
    "print(f\"Shape of Keys: {keys.shape}\")\n",
    "print(f\"Shape of values: {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7877)\n"
     ]
    }
   ],
   "source": [
    "#computing the anttention scores for x-2\n",
    "keys_2 = keys[1]\n",
    "attn_score_2 = query_2.dot(key_2)\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238])\n"
     ]
    }
   ],
   "source": [
    "#we can generalize the computation to all the attention score via matrix multiplication\n",
    "attn_score_2 = query_2 @ keys.T #teh attention scores of all other words in relation to journey\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "#obtaining the attention scores for the whole matrix\n",
    "attn_scores = queries @ keys.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the attn_score matrix shows the attn_score of the ith row in relation to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819])\n"
     ]
    }
   ],
   "source": [
    "#normalizing the attention scores to onbtain the attention weights\n",
    "d_keys = keys.shape[-1]\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_keys ** 0.5, dim=-1)\n",
    "print(attn_weights_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using softmax to normalize the attention scores matrix, we first scale by the square root of the dim of the keys (cols) <br>\n",
    "\n",
    "Reasons we divide with the sqrt of dimension. <br>\n",
    "For stability in learning: the softmax function is sensitive to the magnitudes of the inputs. When the inputs are large, the differnces between the exponential values of each input becomes much more pronounced. this makes the softmax become peaky. Which can make the model overly confident in one particular key. <br>\n",
    "\n",
    "TO make the variance of the dot product stable, multiplying two random numbers, increases the variance, so dividing by sqrt of the dimension keeps the varianc close to 1. the dimension of the vectors directly affects the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3058, 0.8203])\n"
     ]
    }
   ],
   "source": [
    "#obtaining the context vectors \n",
    "context_vector_2 = attn_weights_2 @ values\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genaralizing the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#create a self attention clas\n",
    "class SelfAttention_V1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__() #pareant \n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attm_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5, dim=-1   \n",
    "        )\n",
    "\n",
    "        context_vec = attm_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3076, 0.8176],\n",
      "        [0.3171, 0.8418],\n",
      "        [0.3166, 0.8406],\n",
      "        [0.3003, 0.8048],\n",
      "        [0.2971, 0.7953],\n",
      "        [0.3066, 0.8196]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sam_v1 = SelfAttention_V1(d_in, d_out)\n",
    "print(sam_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using nn.linear function instead of Parameter, which has an optimized weight initilaization scheme, contributing to more stable and effective model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_V2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qlv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qlv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qlv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qlv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vector = attn_weights @ values\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4496,  0.4982],\n",
      "        [-0.4462,  0.4957],\n",
      "        [-0.4462,  0.4956],\n",
      "        [-0.4478,  0.4971],\n",
      "        [-0.4468,  0.4963],\n",
      "        [-0.4479,  0.4971]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sam_v2 = SelfAttention_V2(d_in, d_out)\n",
    "print(sam_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
