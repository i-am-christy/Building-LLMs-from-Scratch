{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#getting the vectors for the input sequence\n",
    "inputs =  torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], #Your x1\n",
    "        [0.55, 0.87, 0.66], #journey x2\n",
    "        [0.57, 0.85, 0.64], #starts x3\n",
    "        [0.22, 0.58, 0.33], #with x4\n",
    "        [0.77, 0.25, 0.10], #one  x5\n",
    "        [0.05, 0.80, 0.55]  #step x6\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining variables\n",
    "x_2 = inputs[2] #second input element, query\n",
    "d_in = inputs.shape[1] #row dimension of the weights matrix, it must be set to the col dimention of the input/query embedding\n",
    "d_out = 2 #col dimension of the weights matrix, it can be set to any thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the query, key and value weight matrices\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) \n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) \n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the `requires_grad` to `False` here means that the values of the weights will not be optimized as the model is being trained. When we are building the model fr, we will set the vaue to true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in GPT models the output dims are usually the same, but for the sake of this practical, we are making them different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)\n",
    "print(W_key)\n",
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4300, 1.4343])\n",
      "tensor([0.4361, 1.1156])\n",
      "tensor([0.3879, 0.9831])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "print(key_2)\n",
    "print(value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Queries: torch.Size([6, 2])\n",
      "Shape of Keys: torch.Size([6, 2])\n",
      "Shape of values: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "#on=btaining the querries, keys and values matrices\n",
    "queries = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(f\"Shape of Queries: {queries.shape}\")\n",
    "print(f\"Shape of Keys: {keys.shape}\")\n",
    "print(f\"Shape of values: {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7877)\n"
     ]
    }
   ],
   "source": [
    "#computing the anttention scores for x-2\n",
    "keys_2 = keys[1]\n",
    "attn_score_2 = query_2.dot(key_2)\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238])\n"
     ]
    }
   ],
   "source": [
    "#we can generalize the computation to all the attention score via matrix multiplication\n",
    "attn_score_2 = query_2 @ keys.T #teh attention scores of all other words in relation to journey\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "#obtaining the attention scores for the whole matrix\n",
    "attn_scores = queries @ keys.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the attn_score matrix shows the attn_score of the ith row in relation to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819])\n"
     ]
    }
   ],
   "source": [
    "#normalizing the attention scores to onbtain the attention weights\n",
    "d_keys = keys.shape[-1]\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_keys ** 0.5, dim=-1)\n",
    "print(attn_weights_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
       "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores / d_keys ** 0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using softmax to normalize the attention scores matrix, we first scale by the square root of the dim of the keys (cols) <br>\n",
    "\n",
    "Reasons we divide with the sqrt of dimension. <br>\n",
    "For stability in learning: the softmax function is sensitive to the magnitudes of the inputs. When the inputs are large, the differnces between the exponential values of each input becomes much more pronounced. this makes the softmax become peaky. Which can make the model overly confident in one particular key. <br>\n",
    "\n",
    "TO make the variance of the dot product stable, multiplying two random numbers, increases the variance, so dividing by sqrt of the dimension keeps the varianc close to 1. the dimension of the vectors directly affects the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3058, 0.8203])\n"
     ]
    }
   ],
   "source": [
    "#obtaining the context vectors \n",
    "context_vector_2 = attn_weights_2 @ values\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genaralizing the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#create a self attention clas\n",
    "class SelfAttention_V1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__() #pareant \n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attm_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5, dim=-1   \n",
    "        )\n",
    "\n",
    "        context_vec = attm_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3076, 0.8176],\n",
      "        [0.3171, 0.8418],\n",
      "        [0.3166, 0.8406],\n",
      "        [0.3003, 0.8048],\n",
      "        [0.2971, 0.7953],\n",
      "        [0.3066, 0.8196]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sam_v1 = SelfAttention_V1(d_in, d_out)\n",
    "print(sam_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using nn.linear function instead of Parameter, which has an optimized weight initilaization scheme, contributing to more stable and effective model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_V2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qlv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qlv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qlv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qlv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vector = attn_weights @ values\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5322, 0.2491],\n",
      "        [0.5316, 0.2488],\n",
      "        [0.5316, 0.2488],\n",
      "        [0.5340, 0.2501],\n",
      "        [0.5331, 0.2497],\n",
      "        [0.5337, 0.2499]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sam_v2 = SelfAttention_V2(d_in, d_out)\n",
    "print(sam_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casual attention mechanism: Here a mask is generated on the attention weights matrix, which turns all the weights above the diagonal to 0 (essentially creates a lower triangular matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#define the context length, which is the length of the input\n",
    "context_length = attn_scores.shape[0]\n",
    "\n",
    "#start masking the attention weights\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying the attention weights matrix with the `mask_simple` diagonal matrix, creates a diagonal matrix where all the weights above the diagonal is equated to zero and can now be used in the casula self attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1500, 0.2264, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1503, 0.2256, 0.2192, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.0000, 0.0000],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.0000],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_matrix = attn_weights * mask_simple\n",
    "masked_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to normalize the `masked_matrix` to make sure that each row sums up to 1, we achieve this by dividing wach row by its sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
      "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_matrix.sum(dim=1, keepdim=True)\n",
    "masked_matrix_norm = masked_matrix / row_sums\n",
    "print(masked_matrix_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of masking is inefficient because the attention score was already calculated with the info mation of the future tokens included.<br>\n",
    "The method below will stop the data leakage issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9231,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [1.2705, 1.8524,   -inf,   -inf,   -inf,   -inf],\n",
       "        [1.2544, 1.8284, 1.7877,   -inf,   -inf,   -inf],\n",
       "        [0.6973, 1.0167, 0.9941, 0.5925,   -inf,   -inf],\n",
       "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707,   -inf],\n",
       "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) #creating an upper triangular matrix leaving out the diagonal\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf) #masked_fill() checks for True values and replace those True values with -inf\n",
    "masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above the influence of the future tokens are masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
       "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing drop out mechanism, with rate of 0.5 that is, we turn off the 50% of each row randomly and rescales the other weights not turned off by a factor of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.ones(6, 6)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2., 2.],\n",
       "        [0., 2., 0., 0., 0., 0.],\n",
       "        [0., 0., 2., 0., 2., 0.],\n",
       "        [2., 2., 0., 0., 0., 2.],\n",
       "        [2., 0., 0., 0., 0., 2.],\n",
       "        [0., 2., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "dropout(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.7366, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.5677, 0.0000, 0.4206, 0.0000, 0.0000],\n",
       "        [0.0000, 0.4727, 0.4662, 0.3639, 0.3068, 0.0000],\n",
       "        [0.3115, 0.4183, 0.0000, 0.0000, 0.2178, 0.3588]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#implementing drop out on the attention weights\n",
    "dropout(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a compact casual attention class, that can handle batches consisting of one than one input.<br> This copact class, does all the masking in one object in one goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the batch with two inputs\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape #dimension of the new batch\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) #tranposing only the inner dimension leaving the outer dimension as it is\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # no of tokens to account for\n",
    "        attn_weights= torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = attn_weights @ values\n",
    "\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Context Vector.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] #counts the number of words in each input batch\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vectors = ca(batch)\n",
    "print(context_vectors)\n",
    "print(f\"Context Vector.shape: {context_vectors.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
